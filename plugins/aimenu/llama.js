// plugins/ai/llama.js

import aiService from '../../lib/ai/index.js';

export default {
  name: "llama",
  commands: ["llama", "llama3", "meta"],
  description: "Chat with Llama 3.3-70b AI model",
  category: "ai",
  usage: "â€¢ .llama <question> - Ask Llama AI anything\nâ€¢ .llama3 <question> - Alternative command",
  
  async execute(sock, sessionId, args, m) {
    try {
      // Validate input
      if (!args[0]) {
        return await sock.sendMessage(m.chat, {
          text: "âŒ Please provide a question!\n\n*Usage:*\n.llama <your question>\n\n*Example:*\n.llama What is machine learning?\n\n> Â© ğ•¹ğ–Šğ–ğ–šğ–˜ ğ•­ğ–”ğ–™"
        }, { quoted: m });
      }

      const query = args.join(' ');

      // Send processing message
      await sock.sendMessage(m.chat, {
        text: `ğŸ¦™ *Llama 3.3-70b is processing...*\n\n_"${query}"_\n\nPlease wait...\n\n> Â© ğ•¹ğ–Šğ–ğ–šğ–˜ ğ•­ğ–”ğ–™`
      }, { quoted: m });

      // Call AI service
      const result = await aiService.llama(query);

      // Handle error
      if (!result.success) {
        return await sock.sendMessage(m.chat, {
          text: `âŒ Llama AI Request Failed!\n\n*Error:* ${result.error.message}\n\n*Tip:* Try .ai or .gpt command instead\n\n> Â© ğ•¹ğ–Šğ–ğ–šğ–˜ ğ•­ğ–”ğ–™`
        }, { quoted: m });
      }

      // Format response
      let response = `ğŸ¦™ *Llama 3.3-70b Response*\n\n`;
      response += `${result.response}\n\n`;
      response += `â° ${result.timestamp}\n`;
      response += `ğŸ¤– Model: ${result.model}\n\n`;
      response += `> Â© ğ•¹ğ–Šğ–ğ–šğ–˜ ğ•­ğ–”ğ–™`;

      // Send response
      await sock.sendMessage(m.chat, {
        text: response
      }, { quoted: m });

      return { success: true };

    } catch (error) {
      console.error("[Llama Plugin] Error:", error);
      await sock.sendMessage(m.chat, {
        text: `âŒ An error occurred!\n\n*Details:* ${error.message}\n\n> Â© ğ•¹ğ–Šğ–ğ–šğ–˜ ğ•­ğ–”ğ–™`
      }, { quoted: m });
    }
  },
};